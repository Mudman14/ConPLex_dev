{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d68c97-2973-4ffe-8bf5-508e19701b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.cuda.set_device(0)\n",
    "\n",
    "BASE_DIR = \"..\"\n",
    "MODEL_BASE_DIR = f\"{BASE_DIR}/best_models\"\n",
    "DATA_DIR = f\"{BASE_DIR}/nbdata\"\n",
    "os.makedirs(MODEL_BASE_DIR,exist_ok=True)\n",
    "os.makedirs(DATA_DIR,exist_ok=True)\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "from plm_dti import DTIDataset, molecule_protein_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2687622d-477e-4c69-a77f-170b413c729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "from tdc import utils\n",
    "from tdc.benchmark_group import dti_dg_group\n",
    "\n",
    "names = utils.retrieve_benchmark_names('DTI_DG_Group')\n",
    "group = dti_dg_group(path = DATA_DIR)\n",
    "benchmark = group.get('bindingdb_patent')\n",
    "name = benchmark['name']\n",
    "train_val, test = benchmark['train_val'], benchmark['test'] ## Natural log transformed (kd/ki/ic50??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bc818e-31da-424f-a3bc-382d21999911",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drugs = pd.concat([train_val,test]).Drug.values\n",
    "all_proteins = pd.concat([train_val,test]).Target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0ea3e4-9793-460c-a54f-ec67506e7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mol_feats import Morgan_f, Morgan_DC_f\n",
    "from prot_feats import Prose_f\n",
    "\n",
    "mol_featurizer = Morgan_DC_f()\n",
    "prot_featurizer = Prose_f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05939ff9-3b12-4442-8fb5-24e4c3b71d15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- precomputing morgan_DC molecule featurizer ---\n",
      "--- loading from disk ---\n",
      "--- precomputing Prose protein featurizer ---\n",
      "--- loading from disk ---\n"
     ]
    }
   ],
   "source": [
    "to_disk_path = f\"{DATA_DIR}/tdc_bindingdb_patent_train\"\n",
    "\n",
    "mol_featurizer.precompute(all_drugs,to_disk_path=to_disk_path,from_disk=True)\n",
    "prot_featurizer.precompute(all_proteins,to_disk_path=to_disk_path,from_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5bc01d9-7ab2-4cb2-b5ed-4ba6735d3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- train your model --- ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f80a5f-5150-43bb-8266-6f0dbfdf398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePLMModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 mol_emb_size = 2048,\n",
    "                 prot_emb_size = 6165,\n",
    "                 hidden_dim = 512,\n",
    "                 activation = nn.ReLU\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.mol_emb_size = mol_emb_size\n",
    "        self.prot_emb_size = prot_emb_size\n",
    "\n",
    "        self.mol_projector = nn.Sequential(\n",
    "            nn.Linear(self.mol_emb_size, hidden_dim),\n",
    "            activation()\n",
    "        )\n",
    "\n",
    "        self.prot_projector = nn.Sequential(\n",
    "            nn.Linear(self.prot_emb_size, hidden_dim),\n",
    "            activation()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(2*hidden_dim, 1)\n",
    "\n",
    "    def forward(self, mol_emb, prot_emb):\n",
    "        mol_proj = self.mol_projector(mol_emb)\n",
    "        prot_proj = self.prot_projector(prot_emb)\n",
    "        # print(mol_proj.shape, prot_proj.shape)\n",
    "        cat_emb = torch.cat([mol_proj, prot_proj],axis=1)\n",
    "        # print(cat_emb.shape)\n",
    "        return self.fc(cat_emb).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "809569a6-1880-4442-96df-98ffffe7c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62eb1c4868874972a0198699c55f4eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Training at Epoch 1 iteration 0 with loss 24.035785675048828\n",
      "[0] Training at Epoch 1 iteration 1000 with loss 5.604084491729736\n",
      "[0] Training at Epoch 1 iteration 2000 with loss 3.7022926807403564\n",
      "[0] Training at Epoch 1 iteration 3000 with loss 1.740458607673645\n",
      "[0] Training at Epoch 1 iteration 4000 with loss 4.7416229248046875\n",
      "[0] Validation at Epoch 1: PCC=0.8190084527118858\n",
      "[0] Training at Epoch 2 iteration 0 with loss 2.08184814453125\n",
      "[0] Training at Epoch 2 iteration 1000 with loss 2.543416976928711\n",
      "[0] Training at Epoch 2 iteration 2000 with loss 1.7525396347045898\n",
      "[0] Training at Epoch 2 iteration 3000 with loss 1.8970952033996582\n",
      "[0] Training at Epoch 2 iteration 4000 with loss 1.8240851163864136\n",
      "[0] Training at Epoch 3 iteration 0 with loss 1.7656350135803223\n",
      "[0] Training at Epoch 3 iteration 1000 with loss 3.5992610454559326\n",
      "[0] Training at Epoch 3 iteration 2000 with loss 2.9975829124450684\n",
      "[0] Training at Epoch 3 iteration 3000 with loss 2.7799134254455566\n",
      "[0] Training at Epoch 3 iteration 4000 with loss 2.04838228225708\n",
      "[0] Training at Epoch 4 iteration 0 with loss 1.5296993255615234\n",
      "[0] Training at Epoch 4 iteration 1000 with loss 1.943325400352478\n",
      "[0] Training at Epoch 4 iteration 2000 with loss 1.6134852170944214\n",
      "[0] Training at Epoch 4 iteration 3000 with loss 1.4224846363067627\n",
      "[0] Training at Epoch 4 iteration 4000 with loss 1.6748132705688477\n",
      "[0] Training at Epoch 5 iteration 0 with loss 0.9880800247192383\n",
      "[0] Training at Epoch 5 iteration 1000 with loss 1.6753605604171753\n",
      "[0] Training at Epoch 5 iteration 2000 with loss 1.5474313497543335\n",
      "[0] Training at Epoch 5 iteration 3000 with loss 1.4972840547561646\n",
      "[0] Training at Epoch 5 iteration 4000 with loss 2.483769655227661\n",
      "[0] Training at Epoch 6 iteration 0 with loss 1.0640349388122559\n",
      "[0] Training at Epoch 6 iteration 1000 with loss 1.3451240062713623\n",
      "[0] Training at Epoch 6 iteration 2000 with loss 1.7231433391571045\n",
      "[0] Training at Epoch 6 iteration 3000 with loss 3.5927326679229736\n",
      "[0] Training at Epoch 6 iteration 4000 with loss 1.216144323348999\n",
      "[0] Validation at Epoch 6: PCC=0.8522854528216004\n",
      "[0] Training at Epoch 7 iteration 0 with loss 2.136143684387207\n",
      "[0] Training at Epoch 7 iteration 1000 with loss 1.9028353691101074\n",
      "[0] Training at Epoch 7 iteration 2000 with loss 2.722984790802002\n",
      "[0] Training at Epoch 7 iteration 3000 with loss 2.0185859203338623\n",
      "[0] Training at Epoch 7 iteration 4000 with loss 2.2339892387390137\n",
      "[0] Training at Epoch 8 iteration 0 with loss 1.2214699983596802\n",
      "[0] Training at Epoch 8 iteration 1000 with loss 0.8175097703933716\n",
      "[0] Training at Epoch 8 iteration 2000 with loss 1.1564379930496216\n",
      "[0] Training at Epoch 8 iteration 3000 with loss 1.9584954977035522\n",
      "[0] Training at Epoch 8 iteration 4000 with loss 1.464493989944458\n",
      "[0] Training at Epoch 9 iteration 0 with loss 2.4222400188446045\n",
      "[0] Training at Epoch 9 iteration 1000 with loss 1.196956992149353\n",
      "[0] Training at Epoch 9 iteration 2000 with loss 2.979642391204834\n",
      "[0] Training at Epoch 9 iteration 3000 with loss 1.2346131801605225\n",
      "[0] Training at Epoch 9 iteration 4000 with loss 2.658158540725708\n",
      "[0] Training at Epoch 10 iteration 0 with loss 0.7828406691551208\n",
      "[0] Training at Epoch 10 iteration 1000 with loss 1.9232029914855957\n",
      "[0] Training at Epoch 10 iteration 2000 with loss 1.0331506729125977\n",
      "[0] Training at Epoch 10 iteration 3000 with loss 0.7686076164245605\n",
      "[0] Training at Epoch 10 iteration 4000 with loss 1.26515793800354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b69a92b1d14b7b9bb5ea2824d5560c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Training at Epoch 1 iteration 0 with loss 36.12716293334961\n",
      "[1] Training at Epoch 1 iteration 1000 with loss 4.674874305725098\n",
      "[1] Training at Epoch 1 iteration 2000 with loss 3.9652135372161865\n",
      "[1] Training at Epoch 1 iteration 3000 with loss 4.16023063659668\n",
      "[1] Training at Epoch 1 iteration 4000 with loss 2.8760318756103516\n",
      "[1] Validation at Epoch 1: PCC=0.8236950453030591\n",
      "[1] Training at Epoch 2 iteration 0 with loss 2.695178985595703\n",
      "[1] Training at Epoch 2 iteration 1000 with loss 2.9995810985565186\n",
      "[1] Training at Epoch 2 iteration 2000 with loss 2.0711474418640137\n",
      "[1] Training at Epoch 2 iteration 3000 with loss 2.877331256866455\n",
      "[1] Training at Epoch 2 iteration 4000 with loss 3.4949283599853516\n",
      "[1] Training at Epoch 3 iteration 0 with loss 3.7171449661254883\n",
      "[1] Training at Epoch 3 iteration 1000 with loss 2.073960304260254\n",
      "[1] Training at Epoch 3 iteration 2000 with loss 1.2705814838409424\n",
      "[1] Training at Epoch 3 iteration 3000 with loss 3.230710506439209\n",
      "[1] Training at Epoch 3 iteration 4000 with loss 2.6267333030700684\n",
      "[1] Training at Epoch 4 iteration 0 with loss 2.0977959632873535\n",
      "[1] Training at Epoch 4 iteration 1000 with loss 2.2821438312530518\n",
      "[1] Training at Epoch 4 iteration 2000 with loss 2.3697450160980225\n",
      "[1] Training at Epoch 4 iteration 3000 with loss 2.1651651859283447\n",
      "[1] Training at Epoch 4 iteration 4000 with loss 1.0353453159332275\n",
      "[1] Training at Epoch 5 iteration 0 with loss 1.2928986549377441\n",
      "[1] Training at Epoch 5 iteration 1000 with loss 1.8554202318191528\n",
      "[1] Training at Epoch 5 iteration 2000 with loss 2.3805947303771973\n",
      "[1] Training at Epoch 5 iteration 3000 with loss 1.3316676616668701\n",
      "[1] Training at Epoch 5 iteration 4000 with loss 0.9031941890716553\n",
      "[1] Training at Epoch 6 iteration 0 with loss 1.1157023906707764\n",
      "[1] Training at Epoch 6 iteration 1000 with loss 2.438920021057129\n",
      "[1] Training at Epoch 6 iteration 2000 with loss 2.0700619220733643\n",
      "[1] Training at Epoch 6 iteration 3000 with loss 1.6057732105255127\n",
      "[1] Training at Epoch 6 iteration 4000 with loss 3.6443843841552734\n",
      "[1] Validation at Epoch 6: PCC=0.8577059180556794\n",
      "[1] Training at Epoch 7 iteration 0 with loss 0.5792372822761536\n",
      "[1] Training at Epoch 7 iteration 1000 with loss 0.7442203760147095\n",
      "[1] Training at Epoch 7 iteration 2000 with loss 1.3018519878387451\n",
      "[1] Training at Epoch 7 iteration 3000 with loss 1.8312995433807373\n",
      "[1] Training at Epoch 7 iteration 4000 with loss 1.2302918434143066\n",
      "[1] Training at Epoch 8 iteration 0 with loss 1.2134089469909668\n",
      "[1] Training at Epoch 8 iteration 1000 with loss 4.820240020751953\n",
      "[1] Training at Epoch 8 iteration 2000 with loss 2.7825841903686523\n",
      "[1] Training at Epoch 8 iteration 3000 with loss 1.478286623954773\n",
      "[1] Training at Epoch 8 iteration 4000 with loss 1.6913807392120361\n",
      "[1] Training at Epoch 9 iteration 0 with loss 2.1920738220214844\n",
      "[1] Training at Epoch 9 iteration 1000 with loss 1.7054089307785034\n",
      "[1] Training at Epoch 9 iteration 2000 with loss 1.0923347473144531\n",
      "[1] Training at Epoch 9 iteration 3000 with loss 1.2047958374023438\n",
      "[1] Training at Epoch 9 iteration 4000 with loss 1.8683764934539795\n",
      "[1] Training at Epoch 10 iteration 0 with loss 0.6934850811958313\n",
      "[1] Training at Epoch 10 iteration 1000 with loss 0.8852128982543945\n",
      "[1] Training at Epoch 10 iteration 2000 with loss 1.4436366558074951\n",
      "[1] Training at Epoch 10 iteration 3000 with loss 0.8884251117706299\n",
      "[1] Training at Epoch 10 iteration 4000 with loss 1.160913109779358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d986c309f6c74d84bd208aec76f15621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] Training at Epoch 1 iteration 0 with loss 29.845439910888672\n",
      "[2] Training at Epoch 1 iteration 1000 with loss 3.7798047065734863\n",
      "[2] Training at Epoch 1 iteration 2000 with loss 6.608565807342529\n",
      "[2] Training at Epoch 1 iteration 3000 with loss 5.181746482849121\n",
      "[2] Training at Epoch 1 iteration 4000 with loss 3.8914988040924072\n",
      "[2] Validation at Epoch 1: PCC=0.8195777564916156\n",
      "[2] Training at Epoch 2 iteration 0 with loss 2.6653618812561035\n",
      "[2] Training at Epoch 2 iteration 1000 with loss 3.062657356262207\n",
      "[2] Training at Epoch 2 iteration 2000 with loss 1.652127981185913\n",
      "[2] Training at Epoch 2 iteration 3000 with loss 1.8174138069152832\n",
      "[2] Training at Epoch 2 iteration 4000 with loss 1.8002684116363525\n",
      "[2] Training at Epoch 3 iteration 0 with loss 1.6508482694625854\n",
      "[2] Training at Epoch 3 iteration 1000 with loss 3.6090269088745117\n",
      "[2] Training at Epoch 3 iteration 2000 with loss 1.437587022781372\n",
      "[2] Training at Epoch 3 iteration 3000 with loss 1.9503026008605957\n",
      "[2] Training at Epoch 3 iteration 4000 with loss 3.2459163665771484\n",
      "[2] Training at Epoch 4 iteration 0 with loss 3.233734607696533\n",
      "[2] Training at Epoch 4 iteration 1000 with loss 1.0860472917556763\n",
      "[2] Training at Epoch 4 iteration 2000 with loss 2.235678195953369\n",
      "[2] Training at Epoch 4 iteration 3000 with loss 2.1223936080932617\n",
      "[2] Training at Epoch 4 iteration 4000 with loss 1.7317496538162231\n",
      "[2] Training at Epoch 5 iteration 0 with loss 1.436310887336731\n",
      "[2] Training at Epoch 5 iteration 1000 with loss 1.1748073101043701\n",
      "[2] Training at Epoch 5 iteration 2000 with loss 1.8459291458129883\n",
      "[2] Training at Epoch 5 iteration 3000 with loss 1.1002168655395508\n",
      "[2] Training at Epoch 5 iteration 4000 with loss 2.7996950149536133\n",
      "[2] Training at Epoch 6 iteration 0 with loss 1.205228328704834\n",
      "[2] Training at Epoch 6 iteration 1000 with loss 1.5799412727355957\n",
      "[2] Training at Epoch 6 iteration 2000 with loss 2.452903985977173\n",
      "[2] Training at Epoch 6 iteration 3000 with loss 1.691168189048767\n",
      "[2] Training at Epoch 6 iteration 4000 with loss 1.1659131050109863\n",
      "[2] Validation at Epoch 6: PCC=0.8540997003155171\n",
      "[2] Training at Epoch 7 iteration 0 with loss 1.167594313621521\n",
      "[2] Training at Epoch 7 iteration 1000 with loss 2.5468950271606445\n",
      "[2] Training at Epoch 7 iteration 2000 with loss 1.1654802560806274\n",
      "[2] Training at Epoch 7 iteration 3000 with loss 1.8377366065979004\n",
      "[2] Training at Epoch 7 iteration 4000 with loss 1.22238028049469\n",
      "[2] Training at Epoch 8 iteration 0 with loss 0.6637245416641235\n",
      "[2] Training at Epoch 8 iteration 1000 with loss 2.667632818222046\n",
      "[2] Training at Epoch 8 iteration 2000 with loss 1.5123858451843262\n",
      "[2] Training at Epoch 8 iteration 3000 with loss 1.4708918333053589\n",
      "[2] Training at Epoch 8 iteration 4000 with loss 1.2634401321411133\n",
      "[2] Training at Epoch 9 iteration 0 with loss 1.6187835931777954\n",
      "[2] Training at Epoch 9 iteration 1000 with loss 0.8983445167541504\n",
      "[2] Training at Epoch 9 iteration 2000 with loss 1.8171679973602295\n",
      "[2] Training at Epoch 9 iteration 3000 with loss 2.161078691482544\n",
      "[2] Training at Epoch 9 iteration 4000 with loss 0.9851251840591431\n",
      "[2] Training at Epoch 10 iteration 0 with loss 1.73210608959198\n",
      "[2] Training at Epoch 10 iteration 1000 with loss 0.9148439764976501\n",
      "[2] Training at Epoch 10 iteration 2000 with loss 1.8851025104522705\n",
      "[2] Training at Epoch 10 iteration 3000 with loss 0.6878113150596619\n",
      "[2] Training at Epoch 10 iteration 4000 with loss 1.477125644683838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554e522ad2284c1a97d33afe21e22ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] Training at Epoch 1 iteration 0 with loss 36.807273864746094\n",
      "[3] Training at Epoch 1 iteration 1000 with loss 3.958169937133789\n",
      "[3] Training at Epoch 1 iteration 2000 with loss 4.124624252319336\n",
      "[3] Training at Epoch 1 iteration 3000 with loss 2.252260446548462\n",
      "[3] Training at Epoch 1 iteration 4000 with loss 2.250272512435913\n",
      "[3] Validation at Epoch 1: PCC=0.8158872440309023\n",
      "[3] Training at Epoch 2 iteration 0 with loss 3.3046488761901855\n",
      "[3] Training at Epoch 2 iteration 1000 with loss 3.400376558303833\n",
      "[3] Training at Epoch 2 iteration 2000 with loss 1.839687466621399\n",
      "[3] Training at Epoch 2 iteration 3000 with loss 4.518401145935059\n",
      "[3] Training at Epoch 2 iteration 4000 with loss 1.9515361785888672\n",
      "[3] Training at Epoch 3 iteration 0 with loss 2.83988094329834\n",
      "[3] Training at Epoch 3 iteration 1000 with loss 1.7946913242340088\n",
      "[3] Training at Epoch 3 iteration 2000 with loss 1.9969806671142578\n",
      "[3] Training at Epoch 3 iteration 3000 with loss 4.6395158767700195\n",
      "[3] Training at Epoch 3 iteration 4000 with loss 3.3169565200805664\n",
      "[3] Training at Epoch 4 iteration 0 with loss 2.7291364669799805\n",
      "[3] Training at Epoch 4 iteration 1000 with loss 2.805389404296875\n",
      "[3] Training at Epoch 4 iteration 2000 with loss 1.705322265625\n",
      "[3] Training at Epoch 4 iteration 3000 with loss 2.243814468383789\n",
      "[3] Training at Epoch 4 iteration 4000 with loss 1.067519187927246\n",
      "[3] Training at Epoch 5 iteration 0 with loss 2.2399134635925293\n",
      "[3] Training at Epoch 5 iteration 1000 with loss 2.0091567039489746\n",
      "[3] Training at Epoch 5 iteration 2000 with loss 1.116166353225708\n",
      "[3] Training at Epoch 5 iteration 3000 with loss 1.5768954753875732\n",
      "[3] Training at Epoch 5 iteration 4000 with loss 5.0264058113098145\n",
      "[3] Training at Epoch 6 iteration 0 with loss 0.7920902967453003\n",
      "[3] Training at Epoch 6 iteration 1000 with loss 1.2851788997650146\n",
      "[3] Training at Epoch 6 iteration 2000 with loss 0.9781047105789185\n",
      "[3] Training at Epoch 6 iteration 3000 with loss 2.8821933269500732\n",
      "[3] Training at Epoch 6 iteration 4000 with loss 0.9488505125045776\n",
      "[3] Validation at Epoch 6: PCC=0.854341184254661\n",
      "[3] Training at Epoch 7 iteration 0 with loss 2.4185798168182373\n",
      "[3] Training at Epoch 7 iteration 1000 with loss 2.9471065998077393\n",
      "[3] Training at Epoch 7 iteration 2000 with loss 0.7207872867584229\n",
      "[3] Training at Epoch 7 iteration 3000 with loss 2.6049516201019287\n",
      "[3] Training at Epoch 7 iteration 4000 with loss 1.7973406314849854\n",
      "[3] Training at Epoch 8 iteration 0 with loss 0.7242438197135925\n",
      "[3] Training at Epoch 8 iteration 1000 with loss 1.1256159543991089\n",
      "[3] Training at Epoch 8 iteration 2000 with loss 1.6423847675323486\n",
      "[3] Training at Epoch 8 iteration 3000 with loss 1.6010068655014038\n",
      "[3] Training at Epoch 8 iteration 4000 with loss 0.7944080829620361\n",
      "[3] Training at Epoch 9 iteration 0 with loss 1.7916772365570068\n",
      "[3] Training at Epoch 9 iteration 1000 with loss 1.5821630954742432\n",
      "[3] Training at Epoch 9 iteration 2000 with loss 1.2561408281326294\n",
      "[3] Training at Epoch 9 iteration 3000 with loss 1.3957303762435913\n",
      "[3] Training at Epoch 9 iteration 4000 with loss 2.244168758392334\n",
      "[3] Training at Epoch 10 iteration 0 with loss 1.019093632698059\n",
      "[3] Training at Epoch 10 iteration 1000 with loss 2.4448912143707275\n",
      "[3] Training at Epoch 10 iteration 2000 with loss 1.6814953088760376\n",
      "[3] Training at Epoch 10 iteration 3000 with loss 1.4686377048492432\n",
      "[3] Training at Epoch 10 iteration 4000 with loss 1.1172120571136475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aaaed55fd40474fb5685d288c545bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Training at Epoch 1 iteration 0 with loss 25.415008544921875\n",
      "[4] Training at Epoch 1 iteration 1000 with loss 2.9821510314941406\n",
      "[4] Training at Epoch 1 iteration 2000 with loss 4.140312194824219\n",
      "[4] Training at Epoch 1 iteration 3000 with loss 2.425964832305908\n",
      "[4] Training at Epoch 1 iteration 4000 with loss 1.9992005825042725\n",
      "[4] Validation at Epoch 1: PCC=0.817678846344615\n",
      "[4] Training at Epoch 2 iteration 0 with loss 2.670764923095703\n",
      "[4] Training at Epoch 2 iteration 1000 with loss 3.197417736053467\n",
      "[4] Training at Epoch 2 iteration 2000 with loss 2.474884033203125\n",
      "[4] Training at Epoch 2 iteration 3000 with loss 4.401054382324219\n",
      "[4] Training at Epoch 2 iteration 4000 with loss 3.0352072715759277\n",
      "[4] Training at Epoch 3 iteration 0 with loss 2.166949510574341\n",
      "[4] Training at Epoch 3 iteration 1000 with loss 1.7536730766296387\n",
      "[4] Training at Epoch 3 iteration 2000 with loss 1.158900260925293\n",
      "[4] Training at Epoch 3 iteration 3000 with loss 2.3164823055267334\n",
      "[4] Training at Epoch 3 iteration 4000 with loss 2.095543146133423\n",
      "[4] Training at Epoch 4 iteration 0 with loss 2.0258116722106934\n",
      "[4] Training at Epoch 4 iteration 1000 with loss 1.1521191596984863\n",
      "[4] Training at Epoch 4 iteration 2000 with loss 1.85301673412323\n",
      "[4] Training at Epoch 4 iteration 3000 with loss 1.6033121347427368\n",
      "[4] Training at Epoch 4 iteration 4000 with loss 2.933671712875366\n",
      "[4] Training at Epoch 5 iteration 0 with loss 1.4786341190338135\n",
      "[4] Training at Epoch 5 iteration 1000 with loss 1.8773736953735352\n",
      "[4] Training at Epoch 5 iteration 2000 with loss 1.9104161262512207\n",
      "[4] Training at Epoch 5 iteration 3000 with loss 2.557492256164551\n",
      "[4] Training at Epoch 5 iteration 4000 with loss 2.750469207763672\n",
      "[4] Training at Epoch 6 iteration 0 with loss 1.5370540618896484\n",
      "[4] Training at Epoch 6 iteration 1000 with loss 1.7270419597625732\n",
      "[4] Training at Epoch 6 iteration 2000 with loss 1.267551302909851\n",
      "[4] Training at Epoch 6 iteration 3000 with loss 1.9714099168777466\n",
      "[4] Training at Epoch 6 iteration 4000 with loss 1.9426980018615723\n",
      "[4] Validation at Epoch 6: PCC=0.8514116607430332\n",
      "[4] Training at Epoch 7 iteration 0 with loss 1.7994346618652344\n",
      "[4] Training at Epoch 7 iteration 1000 with loss 0.9967213869094849\n",
      "[4] Training at Epoch 7 iteration 2000 with loss 1.3369277715682983\n",
      "[4] Training at Epoch 7 iteration 3000 with loss 2.350773334503174\n",
      "[4] Training at Epoch 7 iteration 4000 with loss 1.837847113609314\n",
      "[4] Training at Epoch 8 iteration 0 with loss 1.3436590433120728\n",
      "[4] Training at Epoch 8 iteration 1000 with loss 1.1626800298690796\n",
      "[4] Training at Epoch 8 iteration 2000 with loss 2.260291337966919\n",
      "[4] Training at Epoch 8 iteration 3000 with loss 3.3205246925354004\n",
      "[4] Training at Epoch 8 iteration 4000 with loss 1.0009219646453857\n",
      "[4] Training at Epoch 9 iteration 0 with loss 1.3271074295043945\n",
      "[4] Training at Epoch 9 iteration 1000 with loss 1.8348761796951294\n",
      "[4] Training at Epoch 9 iteration 2000 with loss 2.5443873405456543\n",
      "[4] Training at Epoch 9 iteration 3000 with loss 2.085678815841675\n",
      "[4] Training at Epoch 9 iteration 4000 with loss 1.559748888015747\n",
      "[4] Training at Epoch 10 iteration 0 with loss 2.0587329864501953\n",
      "[4] Training at Epoch 10 iteration 1000 with loss 1.100801706314087\n",
      "[4] Training at Epoch 10 iteration 2000 with loss 1.5770827531814575\n",
      "[4] Training at Epoch 10 iteration 3000 with loss 2.830104351043701\n",
      "[4] Training at Epoch 10 iteration 4000 with loss 1.8607618808746338\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "from time import time\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "test_dataset = DTIDataset(\n",
    "        test.Drug,\n",
    "        test.Target,\n",
    "        test.Y,\n",
    "        mol_featurizer,\n",
    "        prot_featurizer,\n",
    "    )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: molecule_protein_collate_fn(x, pad=False))\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for seed in range(5):\n",
    "    train, valid = group.get_train_valid_split(benchmark = name, split_type = 'default', seed = seed)\n",
    "\n",
    "    train_dataset = DTIDataset(\n",
    "        train.Drug,\n",
    "        train.Target,\n",
    "        train.Y,\n",
    "        mol_featurizer,\n",
    "        prot_featurizer,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: molecule_protein_collate_fn(x, pad=False))\n",
    "\n",
    "    valid_dataset = DTIDataset(\n",
    "        valid.Drug,\n",
    "        valid.Target,\n",
    "        valid.Y,\n",
    "        mol_featurizer,\n",
    "        prot_featurizer,\n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: molecule_protein_collate_fn(x, pad=False))\n",
    "    \n",
    "    # wandb.init(\n",
    "    #         project=args.wandb_proj,\n",
    "    #         name=config.experiment_id,\n",
    "    #         config=flatten(config),\n",
    "    #     )\n",
    "    # wandb.watch(model, log_freq=100)\n",
    "\n",
    "    # early stopping\n",
    "    max_pcc = 0\n",
    "\n",
    "    model = SimplePLMModel().cuda()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    n_epo = 10\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    every_n_val = 1\n",
    "    loss_history = []\n",
    "\n",
    "    tg_len = len(train_dataloader)\n",
    "    start_time = time()\n",
    "    for epo in tqdm(range(n_epo)):\n",
    "        model.train()\n",
    "        epoch_time_start = time()\n",
    "        for i, (d, p, label) in enumerate(train_dataloader):\n",
    "\n",
    "            score = model(d.cuda(), p.cuda())\n",
    "            label = Variable(torch.from_numpy(np.array(label)).float()).cuda()\n",
    "\n",
    "            loss_fct = torch.nn.MSELoss()\n",
    "\n",
    "            loss = loss_fct(score, label)\n",
    "            loss_history.append((epo, i, float(loss.cpu().detach().numpy())))\n",
    "            # wandb.log({\"train/loss\": loss, \"epoch\": epo,\n",
    "            #                \"step\": epo*tg_len*args.batch_size + i*args.batch_size\n",
    "            #           })\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if (i % 1000 == 0):\n",
    "                print(f'[{seed}] Training at Epoch {epo+1} iteration {i} with loss {loss.cpu().detach().numpy()}')\n",
    "\n",
    "        epoch_time_end = time()\n",
    "        if epo % 5 == 0:\n",
    "            with torch.set_grad_enabled(False):\n",
    "                pred_list = []\n",
    "                lab_list = []\n",
    "                model.eval()\n",
    "                for i, (d, p, label) in enumerate(valid_dataloader):\n",
    "                    score = model(d.cuda(), p.cuda())\n",
    "                    score = score.detach().cpu().numpy()\n",
    "                    label = label.detach().cpu().numpy()\n",
    "                    pred_list.extend(score)\n",
    "                    lab_list.extend(label)\n",
    "\n",
    "                pred_list = torch.tensor(pred_list)\n",
    "                lab_list = torch.tensor(lab_list)\n",
    "                val_pcc = pearsonr(pred_list, lab_list)[0]\n",
    "                # wandb.log({\"val/loss\": val_loss, \"epoch\": epo,\n",
    "                #            \"val/pcc\": float(val_pcc),\n",
    "                #            \"Charts/epoch_time\": (epoch_time_end - epoch_time_start)/config.training.every_n_val\n",
    "                #   })\n",
    "                if val_pcc > max_pcc:\n",
    "                    model_max = copy.deepcopy(model)\n",
    "                    max_pcc = val_pcc\n",
    "                print(f'[seed{seed}] Validation at Epoch {epo+1}: PCC={val_pcc}')\n",
    "        end_time = time()\n",
    "        \n",
    "    best_models[seed] = (model_max, max_pcc)\n",
    "    torch.save(model_max, f\"best_models/TDC_DTI_DG_seed{seed}_best_model.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4a9e7a-7cc0-4692-9e3e-b28c802ea73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: PCC=0.504\n",
      "1: PCC=0.518\n",
      "2: PCC=0.505\n",
      "3: PCC=0.506\n",
      "4: PCC=0.529\n",
      "Average PCC: 0.5124000000000001\n"
     ]
    }
   ],
   "source": [
    "pcc_seed = {}\n",
    "\n",
    "for seed in range(5):\n",
    "    pred_list = []\n",
    "\n",
    "    best_mod_ev = best_models[seed][0]\n",
    "    best_mod_ev.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (d, p, label) in enumerate(test_dataloader):\n",
    "            score = best_mod_ev(d.cuda(), p.cuda())\n",
    "            score = score.detach().cpu().numpy()\n",
    "            pred_list.extend(score)\n",
    "\n",
    "    pred_list = np.array(pred_list)\n",
    "    predictions = {name: pred_list}\n",
    "    \n",
    "    out = group.evaluate(predictions)\n",
    "    pcc_seed[seed] = out\n",
    "    print(f'{seed}: PCC={out[name][\"pcc\"]}')\n",
    "print(f'Average PCC: {sum([pcc_seed[s][name][\"pcc\"] for s in range(5)])/5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a2a5c-b1d7-4e0f-b534-214a0d583999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsplat] *",
   "language": "python",
   "name": "conda-env-dsplat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
